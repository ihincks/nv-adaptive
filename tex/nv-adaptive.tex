\documentclass[aps,nofootinbib,twocolumn,superscriptaddress]{revtex4}

\usepackage[strict]{revquantum}


%% PATHS %%

\newcommand{\figurefolder}{../fig}


%% OTHER NOTATION %%

\newcommand{\mps}{x}
\newcommand{\eps}{e}
\newcommand{\data}{d}
\newcommand{\nparticles}{n_\text{p}}
\newcommand{\neps}{n_\text{e}}
\newcommand{\noutcomes}{n_\text{o}}

\newcommand{\MLE}{\text{MLE}}
\newcommand{\ESM}{\text{ESM}}

\newcommand{\Rabi}{\text{Rabi}}
\newcommand{\Ramsey}{\text{Ramsey}}
\newcommand{\te}{t_\text{e}}
\newcommand{\tp}{t_\text{p}}
\newcommand{\tw}{t_\text{w}}
\newcommand{\tm}{t_\text{m}}

\renewcommand{\H}{H}    % hamiltonian
\renewcommand{\L}{L}    % lindblad dissipator
\renewcommand{\S}{S}    % superoperator
\newcommand{\uw}{{\mu\text{w}}}


% http://tex.stackexchange.com/a/112947/615
\newcommand{\apxref}[1]{\hyperref[#1]{Appendix~\ref{#1}}}



%=============================================================================
% FRONT MATTER
%=============================================================================

\begin{document}

\title{Online Bayesian Experiment Design with Quantum Systems}

\author{Ian Hincks}
\affilUWAMath
\affilIQC

\author{Thomas Alexander}
\affilUWPhys
\affilIQC

\author{Michal Kononenko}
\affilTODO

\author{Benjamin Soloway}
\affilTODO

\author{David G. Cory}
\affilUWChem
\affilIQC
\affilPI
\affilCIFAR

% new authors add your names

\date{\today}

\begin{abstract}
Estimating parameters of quantum systems is usually done by performing 
a sequence of predetermined experiments and post-processing the data.
It is known that online design, where the next experiment is based on
the most up-to-date knowledge about the system, 
can offer speedups to parameter estimation.
We apply online Bayesian experiment design a the Nitrogen
Vacancy in diamond to learn the values of a 
seven-parameter model describing its Hamiltonian.
Comparing this to standard pre-determined experiment sweeps, 
we find that we can obtain error bars on some parameters that 
are \TODO~ times smaller given the same amount of data.
This has applications to metrology where making faster measurements
improves sensitivity.
The methods that we use are generic and can in principle be 
applied to any quantum device.
\end{abstract}

\maketitle

%=============================================================================
% MAIN DOCUMENT
%=============================================================================



%=============================================================================
\section{Introduction}
\label{sec:intro}
%=============================================================================

Quantum devices are great.
We'd like to make them better.
Read along to find out how.

%=============================================================================
\section{Inference of Quantum Devices}
\label{sec:inference}
%=============================================================================

We begin by defining some notation while reviewing parameter 
estimation as applied to quantum devices.

Information about a quantum device can be encoded into a list of 
real values, which we call \textit{model parameters}, labeled $\mps$.
For example, in the case of Hamiltonian learning, 
these values parameterize the Hamiltonian operator of the 
quantum system, or in the case of 
state tomography, the entries of a density operator.
This set of parameters includes both parameters of interest, which 
one is interested in learning, and nuisance parameters, which are
not of principle interest, but are still necessary to sufficiently 
describe the system.

Quantum devices are controlled by some collection of 
classical knobs that adjust various settings 
such as power levels, timings, input frequencies, and so on.
We refer to a specific assignment of all of these settings as an
\textit{experiment configuration}, sometimes called the control
variables, which we label $\eps$.
Then an \textit{experiment}
consists of a quantum measurement (or set of quantum measurements) 
made using these fixed experiment configuration.
For example, in this nomenclature, a standard Rabi curve 
would be constructed by making a set of 
experiments, each one defining---among other fixed parameters---a 
pulsing time in its experimental configuration, 
$\eps=(\ldots,t_\text{pulse},\ldots)$.

An experiment returns a datum $\data$. 
This might be a photon count
over a known time interval, a time series of voltages, 
or a number of `down' strong measurement results out of $N$ 
repetitions, and so on.

Generally, the goal of statistical inference is to learn about the parameters
$\mps$ given a data set $\data_1,\ldots,\data_n$ with respective 
configurations $\eps_1,\ldots,\eps_n$.
This requires us to additionally specify a model for the 
system---something which connects the model parameters to the experiment 
configurations and data.
This is done through a likelihood function,
\begin{equation}
    \Lhood(\mps;\data_{1:n},\eps_{1:n})
        = \Pr(\data_{1:n}|\mps,\eps_{1:n}),
    \label{eq:general-likelihood-function}
\end{equation} 
which returns the probability of receiving a given dataset conditioned
on a hypothetical configuration $\mps$.
Here, and throughout this paper, we use subscripted index-range notation,
where, for example, $\data_{1:n}=\{\data_1,...,\data_n\}$.
Note that multiple models can be considered and compared---known
as model selection---if the
true model is not known, and if such a thing even exists.
For quantum systems, these likelihood models come naturally
through quantum system evolution formulas in conjunction 
with Born's rule.

One popular inference choice is to maximize the likelihood function
with respect to $\mps$, producing the maximum likelihood estimate (MLE) 
$\hat{\mps}_\MLE:=\operatorname{argmax}_\mps \Lhood$.
Confidence regions of this estimate can be constructed 
with statistical derivations, or more generally, through techniques like bootstrapping.
Least-squared curve fitting is often used as a proxy for the MLE (with 
confidence intervals arriving from assuming
a linearized model) since it is exactly equal to the MLE
for linear models.

The MLE is one example of an estimator in a vast literature on estimator theory.
In this Letter, we limit ourselves to the use of Bayesian inference because of its
natural integration with online experiments, discussed below.
In short, in the paradigm of (sequential) Bayesian inference, one constantly
maintains a state of knowledge about the model parameters $\mps$, encoded
as a probability distribution $\pi_n(\mps)=\Pr(\mps|\data_{1:n},\eps_{1:n})$, 
where $n=1,2,3,...$ indexes the moment in time when the first $n$ data 
points $\data_{1:n}$ have been collected and processed.
We write $\pi_0(x)$ to denote the distribution prior to all measurements.
The update from $\pi_{n-1}$ to $\pi_n$ is done through Bayes' law,
\begin{equation}
    \pi_n(\eps)
        = \frac{
            \Pr(\data_n|\mps,\eps_n)\pi_{n-1}(\mps)
        }{
            \Pr(\data_n|\eps_n)
        },
\end{equation}
so that our knowledge is improved sequentially as each datum arrives.
Note that this formula is equivalent to 
$\pi_n(\eps)=\Pr(\data_{1:n}|\mps,\eps_{1:n})\pi_0(\mps)/\Pr(\data_{1:n}|eps_{1:n})$ when expanded with the chain rule of conditional probabilites.

%=============================================================================
\section{Experimental Design}
\label{sec:experimental-design}
%=============================================================================

An \textit{experiment design heuristic} is simply a function that 
determines the next experiment configuration to use.
We say such a heuristic is \textit{online} if it explicitly uses the current
state of knowledge to make its choice, and we call it \textit{offline} 
otherwise.
An experiment design timing diagram is shown in \autoref{fig:online-timing-diagram}.
Conventionally, as an example, Rabi curves are generated 
with offline heuristics, where the next experiment is chosen by 
increasing the pulse time by a fixed duration in each experiment.
The number of experiments and pulse time increments are usually 
chosen through Nyquist considerations based on prior implicit
beliefs about the frequencies of the system.

\begin{figure*}
    \includegraphics[width=\textwidth]{\figurefolder/online-timing-diagram}
    \caption{Timing diagram for three rounds of online learning. The role of the
    experiment design heuristic is to pick the next experiment configuration 
    $e_{n+1}$, possibly based
    on the current state of knowledge, $\pi_n(\mps)$, resulting in the
    new data point $d_{n+1}$. 
    This choice of experiment be computationally expensive, and is 
    therefore run concurrently with quantum experiments.}
    \label{fig:online-timing-diagram}
\end{figure*}

We restrict our online design heuristics to Bayesian designs,
summarized in the following framework.
Let $U_n(\mps,\data,\eps)$ be the utility of collecting the datum
$\data$ under configuration $\eps$ given the hypothetical
model parameters $\mps$ and the current state of knowledge $\pi_n(\mps)$.
Using the Bayesian maxim of marginalizing over unknown quantities,
the average utility of observing $\data$ at step $n+1$ under 
the possible experiment configuration $\eps$ is
\begin{align}
    U_n(\data,\eps) 
        &= \int \pi_n(\mps)U(\mps,\data,\eps)\dd\mps.
\end{align}
Since we do not know \textit{a priori} which $\data$ will 
occur (not having performed the experiment yet) the average 
utility of the possible configuration $\eps$ as a whole is
\begin{align}
    U_n(\eps) 
        &= \int\Pr(\data|\eps)U(\data,\eps) \dd\data
\end{align}
where $\Pr(\data|\eps)=\int \Pr(\data|\mps,\eps)\pi_n(\mps)\dd\mps$.
From this quantity we can decide on the next experiment as
the one which maximizes utility,
\begin{align}
    \eps_{n+1} = \argmax_\eps U_n(\eps),
\end{align}
with the maximum taken over some space of possible experiments.
If computed numerically, we might only hope to find local maxima.

One can consider different choices of utility function $U$.
When the application is inference of a non-linear system, such as 
ours, it is typical to choose a utility based on 
mean-squared error.
In particular, we choose $U_n=r_{n,Q}$ where
\begin{align}
    r_{n,Q}(\mps,\data,\eps)
        &= -(\mps - \hat\mps_n)^\T Q (\mps - \hat\mps_n)
\end{align}
and where $Q$ is a positive semi-definite weighting matrix.
Here, $\hat{\mps}_n=\hat{\mps}_n(\data,\eps)=\int \mps \pi_n(\mps)\dd\mps$
is the Bayes estimator assuming $\data$ is received.
In this case, after a bit of rearanging, 
$r_{n,Q}(\eps)$ has the simple interpretation 
as being the negative expected posterior covariance matrix weighted
against $Q$,
\begin{align}
    r_{n,Q}(\eps) = -\Tr Q\expect_\data[\Cov[\mps|\data,\eps]],
    \label{eq:bayes-risk}
\end{align}
a quantity that we call negative \textit{Bayes risk}.

%=============================================================================
\section{Nitrogen Vacancy System Model}
\label{sec:system-model}
%=============================================================================

The quantum system used in our experiment is a nitrogen vacancy (NV) center,
which is a defect found in diamond consisting of a nitrogen adjacent to 
a vacant lattice position.
Our goal for this section is to explicitly define model parameters, experiment configurations, and a likelihood function for this system.
Once this is achieved, we will be able to employ sequential 
Bayesian inference and online experiment design.

When in its stable negatively charged configuration, NV$^-$, 
the vacancy is filled with
six electrons that form an effective spin-1 particle in the optical
ground state---this three level subspace comprises the system of interest.
The eigenstates are labeled $\ket{1}$, $\ket{0}$, and $\ket{-1}$ respectively 
corresponding to the eigenvalues of the spin-1 operator $\Sz=\diag(1,0,-1)$.
There is a zero field splitting (ZFS) of $D\approx\SI{2.87}{GHz}$ 
between $\ket{0}$ and the $\operatorname{span}(\ket{-1},\ket{+1})$ manifold 
that---at low fields ($\lesssim\SI{100}{G}$)---is the dominant energy term,
defining our $z$-axis.
The Zeeman splitting between the states $\ket{-1}$ and $\ket{+1}$ 
is determined by the magnetic field projection onto the $z$-axis, equal to
$\omega_e=\gamma_e |B_z|$ in the secular approximation, 
where $\gamma_e\approx\SI{2.80}{MHz/G}$.
Spin manipulation is achieved with resonant microwave driving near
the transitions $D\pm\omega_e$.
Long coherence times are observed at room temperature, where
the spin state can be initialized and measured optically, and single 
defects are studied in isolation using confocal microscopy.

In the rotating frame, with the rotating wave and secular approximations,
the Hamiltonian of the optical ground state is given by
\begin{align}
    \H/2\pi &= (D-\omega_\uw)\Sz^2 + (\omega_e+A \Iz)\Sz + \Omega_1(t)\Sx
\end{align}
where $(\Sx,\Sy,\Sz)$ are the spin-1 operators, $\omega_\uw$ is the applied
microwave frequency, $\Omega_1(t)$ is the microwave drive strength, $A$ is
the hyperfine splitting  due to the adjacent nitrogen-14 atom, and $\Iz$
is the nitrogen spin-1 operator along $z$.
Along with the $T_2^*$ decoherence time which introduces the 
Lindblad operator $\L=\sqrt{1/T_2^*}\Sz$, these parameters are sufficient
to simulate the experiments that we perform.
Therefore, the model parameters of our spin system (two
more nuisance parameters will be added later) are given by
\begin{align}
    \mps=(\Omega,\omega_e,\delta D,A,(T_2^*)^{-1})
\end{align}
where $\delta D=D-\SI{2.87}{GHz}$ and $\Omega$ is the maximum possible
value that $\Omega_1(t)$ can take.

A general experiment configuration is specified by
\begin{align}
    \eps=(a(t), \omega_\uw, N)
\end{align}
where $a(t):[0,\te]\rightarrow [-1,1]$ is the 
unitless pulse profile yielding $\Omega_1(t)=a(t)\Omega$,
$\omega_\uw$ is the applied microwave frequency,
and $N$ is the number of repetitions of this experiment\footnote{The
experiment configuration must also specify value for each of the timings
labeled in \autoref{fig:experiment-pulse-sequences}, but as they are
calibrated independently from the experiment of interest, we omit
them here for simplicity.}.
In this paper, we restrict our attention to two special
cases of this general form, depicted in 
\autoref{fig:experiment-pulse-sequences}, given by
\begin{enumerate}
    \item Rabi experiments, $\eps_\Rabi=(\tp,\omega_\uw, N)$, 
    $a(t)=1$ for all $0\leq t\leq \te=\tp$; and
    \item Ramsey experiments, $\eps_\Ramsey=(\tp,\tw,\omega_\uw, N)$, 
    \begin{equation*}
        a(t)=\begin{cases}
            0 & \tp<t<\tp+\tw\\
            1 & \text{else}
        \end{cases}
    \end{equation*}
    for all $0\leq t\leq \te=2\tp+\tw$.
\end{enumerate}

Given a hypothetical set of model parameters $\mps$ and 
an experiment configuration $\eps$, the superoperator
(in column-stacking convention) is given by the solution
to the GKSL master equation,
\begin{subequations}
\begin{align}
    S(\mps,\eps)
        &=\mathcal{T}\e^{\int_0^{\te}(C[\H(t)]+D[\L])\dd t} \\
    C[\H(t)]
        &=-i(\I\otimes\H(t)-\overline{\H(t)}\otimes\I) \\
    D[L] 
        &= \overline{L}\otimes L
            -(\I\otimes L^\dagger L + \overline{L^\dagger L}\otimes \I) / 2
\end{align}
\end{subequations}
where $\mathcal{T}$ is Dyson's time ordering symbol.
This results in the measurement probability
\begin{align}
    p(\mps,\eps)
        &=  \dbra{P_0} \S(\mps,\eps)\dket{\rho_0}
\end{align}
where our initial state is $\rho_0=\ketbra{0}\otimes\I/3$
and the measurement projector is $P_0=3\rho_0$.

The standard measurement protocol of the NV system at room
temperature does not have direct access to strong measurements..
Instead, the probability $p(\mps,\eps)$ is obstructed by three Poisson
rates, so that data is in the form of a triple $\data=(X,Y,Z)$
where
\begin{subequations}
\begin{align}
    X|\alpha
        & \sim\poissondist(N\alpha) \\
    Y|\beta
        & \sim\poissondist(N\beta) \\
    Z|\mps,\eps,\alpha,\beta
        & \sim\poissondist(N(\beta+p(\mps,\eps)(\alpha-\beta)))
\end{align}
\label{eq:nv-measurement}
\end{subequations}
where $\alpha$ and $\beta$ satisfy $0<\beta<\alpha$ and 
are the number of expected
photons for the bright and dark references in a single shot with a 
given measurement duration $\tm$, respectively.
The values $\alpha$ and $\beta$ are nuisance parameters which we
must append to our model parameters, giving
\begin{align}
    \mps=(\Omega,\omega_e,\delta D,A,(T_2^*)^{-1},\alpha,\beta).
    \label{eq:nv-model-parameters}
\end{align}

The likelihood function (see \autoref{eq:general-likelihood-function}) 
for a single experiment is then given by
\begin{align}
    \Lhood(\mps;\data,\eps) 
        &= f(X,N\alpha)
            \cdot f(Y, N\beta) \nonumber \\
            &\quad\quad\times f(Z, N(\beta+p(\mps,\eps)(\alpha-\beta))
    \label{eq:likelihood-function}
\end{align}
where $f$ is the probability mass function of the Poisson distribution,
$f(Q,\lambda) = \e^{-\lambda}\lambda^Q/Q!$.

\begin{figure}[t]
    \includegraphics[width=\textwidth]{\figurefolder/experiment-pulse-sequences}
    \caption{Pulse timing diagrams for Rabi (top) and Ramsey 
        (bottom) experiments. An experiment has three control lines:
        whether the laser is on or off, whether the APD is counting 
        photons or not, and the microwave amplitude profile.
        The pulse sequence is repeated $N$ times, collecting
        photon counts $(X_i,Y_i,Z_i)$ for $i=1,...,N$ for the bright reference,
        dark reference, and experiment, respectively, and finally summing them
        each over $i$ to produce the data point $\data=(X,Y,Z)$.
        Initial states are prepared by lasing for time $t_\text{r}$
        and letting the system settle for time $t_\text{s}$.
        Measurements consist of detecting photons for 
        durations of length $\tm$ while lasing.
        The dark reference includes an adiabatic pulse of length 
        $t_\text{a}$ which effects $\ket{0}\rightarrow\ket{+1}$.
        The action of interest implements the microwave envelope 
        $\Omega_1(t)$ of duration $\te$, 
        which for a Rabi experiment is a square pulse ($\te=\tp$),
        and for a Ramsey experiment is a delay between two equal
        length square pulses ($\te=2 \tp+\tw$).
        }
    \label{fig:experiment-pulse-sequences}
\end{figure}

%=============================================================================
\section{Implementation}
\label{sec:implementation}
%=============================================================================

\subsection{Hardware}

\subsection{Numerics and Computation}

For all experiment design heuristics, offline and online, 
we use the sequential Monte Carlo (SMC\footnote{This is
sometimes also called sequential importance sampling and resampling (SIS/R)})\citeneed method to numerically compute sequential posteriors
using the Python library qinfer\cite{granade_qinfer:_2017}.
In this algorithm, the state of knowledge about the model 
parameters, $\pi_n(\mps)$, 
is approximated as a finite list of weighted hypothetical values
(which are called \textit{particles}),
\begin{align}
    \pi_n(\mps) 
        &= \sum_{i=1}^{K}w_{n,i} \delta(\mps-\mps_{n,i}),
    \label{eq:particle-approximation}
\end{align}
where $w_{n,i}\geq 0$ with $\sum_{i=1}^K w_{n,i}=1$, and where
$\delta(\cdot)$ is the delta mass distribution centered at $0$.
The particle-approximated prior, $\pi_0(\mps)$, is generated by 
sampling $K$ initial particles $\mps_{0,i}$ from the prior distribution
and setting uniform weights $w_{0,i}=1/K$.
Given the new datum $\data_{n+1}$ under experiment 
configuration $\eps_{n+1}$,
Bayes' update can be implemented with the simple multiplication 
\begin{align}
    w_{n+1,i}
        &\propto w_{n,i}\cdot \Lhood(\mps_{n,i};\data_{n+1},\eps_{n+1})
    \label{eq:particle-update}
\end{align}
which requires $K$ simulations of the quantum system to
compute the likelihoods (\autoref{eq:likelihood-function}), and where
the constant of proportionality is chosen so that 
$\sum_{i=1}^K w_{n+1,i}=1$.
We use the scheme of Liu and West to resample particle locations, 
triggered by a threshold in the effective particle count,
$n_\text{eff}:=1/\sum_{i=1} w_{n,i}^2$.
We also use the bridged-updating trick discussed in
Reference~\cite{hincks_statistical_2018}.

Notice that the expensive stage of this algorithm is 
embarrassingly parallel---simulations under the various
model parameters $\mps_{n,i}$ can be performed independently.
All of our processing was run on a desktop computer with simulations
parallelized over the 12 cores in a pair of Intel Xeon X5675 CPUs.
In this configuration, our updates took on the order of $2$ seconds
with $K=30000$\todo{verify}.
In principle, simulations could instead be run on quantum simulators, 
as was recently demonstrated\cite{wang_experimental_2017}.

For online heuristics, Bayes risk (\autoref{eq:bayes-risk}) 
is calculated by noting that the 
particle approximation turns all integrals, which includes
expectations and covariances, into finite sums;
details are discussed in appendix\TODO.
As seen in \autoref{fig:online-timing-diagram}, these 
calculations (along with the Bayes updates) are
performed concurrently with experiments so that they do not 
add to experiment cost\footnote{Of course, this is only
possible so long as the experiment repetition count is large 
enough compared to the parallelized simulation cost. In our setup, at
our count rates, we landed naturally in this regime with
a single desktop computer.}.
This has the side-effect of the next experiment being chosen with
information that is one cycle out-of-date; however,
in our simulations we found that this did not have a noticeable
effect on learning rates.
A new experiment configuration $\eps_{n+1}$ having been 
decided, by whatever heuristic, 
the processing computer sends $\eps_{n+1}$ to the 
computer which controls experiments.
The experiment is run, and the datum $\data_{n+1}=(X,Y,Z)$
is returned to the processing computer.

We use a custom built confocal microscope to isolate an individual
NV center in bulk diamond.
All experiments are performed on the same NV center.
Microwave power is delivered to the defect with a nearby antenna
Laser light is produced by a continuous-wave \SI{100}{mW} laser
at \SI{532}{nm}, and switched using a double pass through an
acousto-optic modulator (Isomet 1250C-84).
Photons are collected with an avalanche-photo detector (APD; Excelitas SPCM-AQRH)
Microwaves are transmitted by an antenna of diameter \SI{25}{um} 
about \SI{100}{um} away from the defect

Experimental configurations are stored as waveforms on an arbitrary waveform generator (Tektronix AWG5002).

\subsection{Effective strong measurements and drift tracking}

The amount of information provided by a measurement of $Z$ (see
\autoref{eq:nv-measurement}) depends on values 
of $\alpha$ and $\beta$.
Their magnitudes, relative contrast, and uncertainty 
all contribute to this information content.
We quantify this idea by introducing what we call
the number of \textit{effective strong measurements} (ESM), 
$n_\ESM$,
defined as the number of strong measurements one would
(hypothetically) have to do to gain the equivalent amount of 
information about $p(\mps,\eps)$, averaged uniformly over $p\in[0,1]$.
This works out to
\begin{align}
    n_\ESM = \frac{
            (\hat\alpha-\hat\beta)^2
        }{
            3(\hat\alpha+\hat\beta)+2\left(\sigma_\alpha^2+\sigma_\beta^2\right)
        }.
\end{align}
where $\hat\alpha$ and $\hat\beta$ are our current estimates of
$\alpha$ and $\beta$, and $\sigma_\alpha$ and $\sigma_\beta$ 
are our standard deviation uncertainties in these estimates.
See \autoref{apx:effective-strong-measurements} for details.
We choose the number of repetitions in the next experiment, $N$, 
such that the expected value of $n_\ESM$ is constant 
(see \autoref{fig:tracking-example}(b-c)).
This is especially important for the purpose of our paper,
which is to compare experiment design heuristics.
In this way,
certain heuristics are not artificially improved because
of favorable lab conditions on a certain day of the week.

The true specific values of the references $\alpha$ and $\beta$ depend 
not only on the optical dynamics of the quantum system itself,
but also on the quality of the microscope's alignment.
As the temperature of the lab changes, for instance, one
can expect the values of $\alpha$ and $\beta$ to change as well
as the location of the NV center drifts with respect to the
focal region of the microscope.
To account for this, a tracking operation is
performed periodically, where the focus of microscope is
repositioned based on a new set of images taken with the microscope.

A model that assumes these reference values are constant 
in time can lead to inaccurate results or even failure.
To account for this drift, 
we append a zero-mean Gaussian random
walk model on the parameters $\alpha$ and $\beta$ to the static
model defined in \autoref{sec:system-model}.
Specifically, we assume that immediately prior to 
a particle update (\autoref{eq:particle-update}) the reference
indices of the each model parameter particle
undergoes a resampling step defined as
\begin{align}
    \matrixtwobyone{\alpha_{n,i}}{\beta_{n_i}}
        &\sim \normaldist\left(
            \matrixtwobyone{\alpha_{n,i}}{\beta_{n_i}},
            \Delta t 
            \matrixtwobytwo{\sigma_\alpha^2}{\sigma_{\alpha,\beta}}{\sigma_{\alpha,\beta}}{\sigma_\beta^2}
            \right)
\end{align}
where $\Delta t$ is the amount of time elapsed since the last
update.
The hyper-parameters $\sigma_\alpha$, $\sigma_\beta$, and 
$\sigma_{\alpha,\beta}$ are treated as unknown, appended to
the model parameters, co-learned along with the parameters defined
in \autoref{eq:nv-model-parameters}.
We use a wide inverse Wishart distribution as the prior with a 
degrees-of-freedom parameter $\nu=30$ and a scale matrix $\Psi$
such that the mean value of the prior corresponds to
$\sigma_\alpha=\sigma_\beta=\sigma_{\alpha,\beta}/0.7=\SI{0.01}{/ms}$.
When a tracking operation is performed, the distribution of 
$\alpha$ and $\beta$ is resampled from the prior $\pi_0(\mps)$,
with all other parameters of the model held fixed.
We chose to perform tracking operation at the start of each trial,
and each time our estimate of $\alpha$ dipped below
our prior estimate of $\alpha$ minus five times the standard
deviation of our prior for $\alpha$.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{\figurefolder/tracking-example}
    \caption{An NV drift tracking example, where tracking 
    operations take place at the vertical dashed lines.
    (a) Sub-poissonian $95\%$ credible regions are shown on top of data
    normalized against the experiment repetition count, $N$.
    (b) The repetition count was chosen online to maintain a
    constant ESM value of $20$, which is plotted in (c).
    regions are shown. Several hundred trials were searched through
    to find this extreme example---references are typically
    quite flat.}
    \label{fig:tracking-example}
\end{figure}

%=============================================================================
\section{Results}
\label{sec:results}
%=============================================================================

We implement  \autoref{tab:heuristics}.

\newcommand{\mystrut}{\rule{0pt}{1.5\normalbaselineskip}}
\begin{table*}[t]
    \centering
    \begin{tabularx}{\textwidth}{m{15em}X}
        \textbf{Heuristic} & \textbf{Definition} \\
        \hline\mystrut
        Alternating Linear
            & Sequential alternation between elements of 
                the experiment \\ 
                & sets
                $E_\Rabi(\SI{500}{ns},100)$ and 
                $E_\Ramsey(\hat{t}_{\text{p,best}},\SI{2}{us},100)$ \\
        \hline\mystrut
        Uniformly Weighted Risk 
            & $e_{n+1}=\underset{e\in E}{\argmax}\left(r_{n,Q}(e)\right)$
                where $Q=\diag(1,1,1,1,1,0,0)$ and \\
                & $E=E_\Rabi(\SI{500}{ns},100)\cup
                E_\Ramsey(\hat{t}_{\text{p,best}},\SI{2}{us},100)$ \\
        \hline\mystrut
        Magnetometry Focused Risk
            & $e_{n+1}=\underset{e\in E}{\argmax}\left(r_{n,Q}(e)\right)$
                    where $Q=\diag(0,1,0,0,0,0,0)$ and \\
                    & $E=E_\Rabi(\SI{500}{ns},100)\cup
                    E_\Ramsey(\hat{t}_{\text{p,best}},\SI{2}{us},100)$ \\
        \hline
    \end{tabularx}
    \caption{Summary of heuristics used to choose experiments.
        The best Ramsey tip time is defined by
        $\hat{t}_{\text{p,best}}=1/(4\hat{\Omega})$ (rounded to the nearest
        \SI{2}{ns}), where $\hat\Omega$ is the current
        Bayes estimate of the microwave drive amplitude.
        $E_\Rabi(t_{\max},m)$ denotes a set of Rabi experiments with pulse 
        times $\tp=t_{\max}/m,2t_{\max}/m,\ldots,t_{\max}$, and 
        $E_\Ramsey(\tp, t_{\max},m)$ denotes a set of Ramsey experiments with wait times
        $\tw=t_{\max}/m,2t_{\max}/m,\ldots,t_{\max}$ and pulse times $\tp$.
    }
    \label{tab:heuristics}
\end{table*}   

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{\figurefolder/heuristic-comparison}
    \caption{Comparison of heuristics.}
    \label{fig:heuristic-comparisino}
\end{figure}

%=============================================================================
\section{Conclusions}
\label{sec:conclusions}
%=============================================================================


%=============================================================================
% END MATTER
%=============================================================================

\acknowledgments{
    
}

\bibliographystyle{apsrev4-1}
\bibliography{nv-adaptive}

%=============================================================================
% APPENDICES
%=============================================================================

\appendix
\onecolumngrid

%=============================================================================
\section{Effective Strong Measurements}
\label{apx:effective-strong-measurements}
%=============================================================================

Given a quantum state $\rho$, 
information is accessed through the
Born's probability $p=\Tr(\ketbra{0}\rho)$.
In the hypothetical case of strong measurement, in the language
of statistics, we would be able to draw from 
the Bernoulli distribution $\bernoullidist(p)$, or more generally, with 
$n$ repeated preparations and strong measurements, from 
the binomial distribution $\binomialdist(n,p)$.

Standard room temperature NV setups do not allow strong measurements. 
Instead, access to the quantity $p$ is obstructed by three Poisson rates,
such that conditional on some values $0<\beta<\alpha$, we can 
draw from the random variables
\begin{align}
    X|\alpha &\sim \poissondist(\alpha) \nonumber \\
    Y|\beta &\sim \poissondist(\beta) \nonumber \\
    Z|\alpha,\beta,p &\sim \poissondist(p\alpha + (1-p)\beta).
\end{align}
The quantities $\alpha$ and $\beta$ are known as the bright reference
and the dark reference, respectively.
They are defined as the 
expected number of photons collected (and summed over $N$
repetitions of the experiment) using the initial NV states
$\ketbra{0}$ and $\ketbra{1}$, respectively\footnote{They are
more accurately defined in terms of the pseudo-pure states 
that are actually created in the NV initialization procedure~\cite{hincks_statistical_2017}.}.

The information content about $p$ of this referenced Poisson model is not
immediately obvious, 
and depends both on the magnitude of $\alpha+\beta$, 
as well as the contrast between $\alpha$ and $\beta$.
This is different than the strong measurement case mentioned above,
where $n$ strong measurements has a clear intuitive and operational 
interpretation.
The goal of this section is to reduce information about the references
$\alpha$ and $\beta$ into a single number 
with the same interpretation as $n$.
This will allow one, for example,
to quantitatively compare two experimental setups or NVs and
decide which one is better at providing information about $p$. 

It has been shown\cite{hincks_statistical_2017} that the 
Fisher information matrix of this 
referenced Poisson model is given by
\begin{align}
    J(p,\alpha,\beta) &=
        \begin{pmatrix}
            \frac{(\alpha -\beta )^2}{p (\alpha -\beta )+\beta } & 
            \frac{p (\alpha -\beta )}{p (\alpha -\beta )+\beta } & 
            \frac{\alpha }{\beta +\alpha  p-\beta  p}-1 \\
            \frac{p (\alpha -\beta )}{p (\alpha -\beta )+\beta } & 
            \frac{p^2}{p \alpha -p \beta +\beta }+\frac{1}{\alpha } & 
            -\frac{(p-1) p}{p (\alpha -\beta )+\beta } \\
            \frac{\alpha }{p \alpha -p \beta +\beta }-1 & 
            -\frac{(p-1) p}{p (\alpha -\beta )+\beta } & 
            \frac{p \alpha +(p-2) (p-1) \beta }{\beta  (p (\alpha -\beta )+\beta )} \\
        \end{pmatrix}
\end{align}
with inverse matrix
\begin{align}
    J(p,\alpha,\beta)^{-1}
        &=
        \begin{pmatrix}
            \frac{p (p+1) \alpha +(p-2) (p-1) \beta }{(\alpha -\beta )^2} & 
            \frac{p \alpha }{\beta -\alpha } & 
            \frac{(p-1) \beta }{\alpha -\beta } \\
            \frac{p \alpha }{\beta -\alpha } & 
            \alpha  & 
            0 \\
            \frac{(p-1) \beta }{\alpha -\beta } & 
            0 & 
            \beta
        \end{pmatrix}.
\end{align}

Using the Cramer-Rao bound, these matrices let us
estimate the information content of $p$ in the referenced
Poisson model.
Specifically, they give us an estimate in each of the following 
extreme cases.
First, the $(p,p)$ element of $J^{-1}$, 
$(J^{-1})_{p,p}=\frac{p (p+1) \alpha +(p-2) (p-1) \beta }{(\alpha -\beta )^2}$,
is a lower bound on the variance of any (unbiased) estimate of $p$ given that
a single measurement of the triple $(X,Y,Z)$ has been made, 
with no prior information
about $p$, $\alpha$, or $\beta$ given.
Second, the inverse of the $(p,p)$ element of $J$,
$(J_{p,p})^{-1}=\frac{p (\alpha -\beta )+\beta}{(\alpha -\beta )^2 }$,
 is a lower bound on 
the variance of any (unbiased) estimate of $p$ given that a single 
measurement of $Z$ has been made, assuming perfect knowledge of
both $\alpha$ and $\beta$.

It will be useful for us to also be able to interpolate between these two
extremes, where some, but not all, prior information 
about $\alpha$ and $\beta$ is available.
There are a few tacks that one might consider to achieve this, including
the Bayesian Cramer-Rao bound, or looking directly at the risk of some 
estimator.
Instead, we choose a slightly ad-hoc method as it actually produces 
a tractable calculation---statistics of the referenced Poisson model
usually involve a triple infinite sum, and many calculations are simply
not possible without numerics.
To this end, let $\sigma_\alpha^2$ and $\sigma_\beta^2$ represent 
our prior variances of $\alpha$ and $\beta$, respectively, before
taking a measurement of $Z|\alpha,\beta,p$.
We can now ask the question: how many times, $M$, we must measure 
$X|\alpha$ and $Y|\beta$ to produce these variances in the first 
place?
We must allow $M$ to depend on $\alpha$ or $\beta$ in each case.
The distribution $\poissondist(M(\lambda)\lambda)$ has 
Fisher information given by 
$\frac{(M(\lambda)+\lambda M'(\lambda))^2}{\lambda M(\lambda}$.
Equating this to $1/\sigma^2$ and soliving the differential
equation at $M(0)=0$ gives $M=\lambda/4\sigma^2$.
Therefore consider the distribution
\begin{align}
    \poissondist\left(\frac{\alpha^2}{4\sigma_\alpha^2}\right)
        \times \poissondist\left(\frac{\beta^2}{4\sigma_\beta^2}\right)
        \times \poissondist\left(p\alpha+(1-p)\beta\right)
\end{align}
which effectively results in our desired information about $\alpha$
and $\beta$.
Solving for the $(p,p)$ element of the inverse Fisher information
matrix of this distribution results in the formula
\begin{align}
    K=\frac{
        \beta +p\left(\alpha-\beta+p \sigma_\alpha ^2
        +(p-2) \sigma_\beta ^2\right)+\sigma_\beta ^2}{(\alpha -\beta )^2}.
\end{align}
This formula correctly interpolates between the case of perfect prior 
information, and prior information as collected by a single sample
of $(X,Y)|\alpha,\beta$, namely,
\begin{align}
    \lim_{\sigma_\alpha^2,\sigma_\beta^2\rightarrow 0} K 
        &= (J_{p,p})^{-1} \quad\quad\text{and}\quad\quad
    \lim_{\sigma_\alpha^2\rightarrow \alpha, \sigma_\beta^2\rightarrow \beta} K 
        = (J^{-1})_{p,p}.
\end{align}

The inverse Fisher information of the binomial model $\binomialdist(n,p)$
is given by $\frac{p(1-p)}{n}$, which when integrated uniformly over 
$[0,1]$, produces $\frac{1}{6n}$.
Our definition for the number of effective strong measurements (ESM)
of a referenced Poisson model with parameters 
$(\alpha,\beta,\sigma_\alpha,\sigma_\beta)$ is defined by 
equating $\int_0^1 K\dd p=\frac{1}{6n}$ and solving for $n$,
which results in
\begin{align}
    n_\ESM = \frac{
            (\alpha-\beta)^2
        }{
            3(\alpha+\beta)+2\left(\sigma_\alpha^2+\sigma_\beta^2\right)
        }.
\end{align}
This shows, for example, 
that having perfect information about $\alpha$ and $\beta$
before measuring $Z|\alpha,\beta,p$ is roughly equivalent---in terms
of information learned about p---to 
$5/3\approx 1.67$ times more effective strong measurements 
than the case where the triple $(X,Y,Z)|\alpha,\beta,p$ is
measured, but with no prior information.

Finally, in \autoref{fig:effective-strong-meas}, we use some numerics
to show that the $\ESM$
quantity accurately relates the mean-squared error of the Bayes estimator
for the referenced Poisson model and a binomial model with $n=n_\ESM$.

\begin{figure}
    \includegraphics[width=\textwidth]{\figurefolder/effective-strong-meas}
    \caption{The mean-squared-error of the Bayes estimator is computed
    as a function of $p$
    for both the referenced Poisson model (blue, solid) and for a binomial model
    (orange, dashed) where $n=n_\ESM$.
    The prior distribution on $p$ is uniform.
    This is done in nine regimes, corresponding to the nine subplots of the figure.
    Each row uses a different magnitude of bright reference, $\alpha$,
    and each column uses a different amount of prior reference knowledge.
    The left column uses sub-Poisson error bars on $\alpha$ and $\beta$,
    and the right column uses regular Poisson error bars.}
    \label{fig:effective-strong-meas}
\end{figure}

%=============================================================================
\section{Simulation the Nitrogen}
\label{apx:nitrogen-sim}
%=============================================================================

The nitrogen atom adjacent to the vacancy couples to our system with
an isotropic hyperfine interaction, $A\cdot\Sz\otimes\Iz$.
Therefore, for a given experiment, we treat it as a small fixed magnet
with strength $m_I A$, where $m_I\in\{-1,0,+1\}$ is its current eigenstate,

This is justified due to its long $T_1$ time with respect to a single
experiment---in 

%=============================================================================
\section{Computing the Bayes Risk}
\label{apx:computing-bayes-risk}
%=============================================================================

In online Bayesian heuristics, hypothetical posteriors are 
typically used to decide which experiment to run next.
For example, the squared error risk of experiment $\eps$ can
be written as the posterior variance marginalized over all data
that the current prior deems relevant:
\begin{align}
    r(\eps)
        &= \expect_\data \Var_Q[x|\eps,\data] \nonumber \\
        &= \int \Pr(\data|\eps) 
            \int \Pr(\mps|\data,\eps)
            (\mps-\hat{\mps}(\data,\eps))^\T \cdot Q\cdot
                (\mps-\hat{\mps}(\data,\eps))
                \dd\mps \dd\data
    \label{eq:mse-risk}
\end{align}
Here, $Q$ is a positive semi-definite weight matrix,
$\Pr(\data|\eps)=\int\Pr(\data|\mps,\eps)\Pr(\mps)\dd\mps$ is the total
probability of $\data$ under the current prior given experiment $\eps$, and
$\hat\mps(\data,\eps)$ is the Bayes estimator of $\mps$ given
hypothetical data $\data$ under experiment $\eps$, i.e.,
\begin{align}
    \hat\mps(\data,\eps) = \int \mps\Pr(\mps|\data,\eps)\dd\mps.
\end{align}

Integrals becomes sums if relevant parameters are discrete, or,
as is the case with our implementation, if distributions are stored
as finite lists of weighted particles.
The value of \autoref{eq:mse-risk} becomes very expensive to compute,
considering also that we need to calculate it for many candidate
experiments $\eps$.
If distributions over $\mps$ are stored using $\nparticles$ particles,
and $\neps$ experimental configurations are of interest, 
and $\noutcomes$ outcomes are relevant, then
we require $\neps\cdot\nparticles\cdot\noutcomes$ 
calculations of the model's likelihood function.
The values $\neps$ and $\nparticles$ can only be lowered so far;
we need enough $\nparticles$ to effective store a distribution,
and the landscape of $r(\eps)$ is non-trivial, so that looking at many 
values of $\neps$ is required even if we are being more clever than
brute-force.

There headway to make some improvement on $\noutcomes$, however.
For the NV center, outcomes are Poisson counts, which
can be any non-negative integer.
Obviously this should be restricted to only those which are 
relevant, for example, all integers between the dark
reference and the bright reference with some standard
deviations added on.
This is about as good as we can hope for if we explicitly
sum over outcomes in our risk evaluation.
In the regime we operated our experiment, this scheme incurs roughly 
$\noutcomes=500$, and gets worse with better NV optics, or
more experiment repetitions. 


We can make more notable improvements if we notice
that we are not interested in the actual values of $r(e)$,
just in the relative value with respect to many experiments
$e$; we want to choose the experiment which minimizes risk.
Therefore we can settle on some stand-in function
for $r(e)$ that hopefully shares the same minimum location.

The simplest is to imagine that we are making single strong measurements
instead of collecting data through a referenced Poisson model.
In this case, there are two outcomes $d\in\{0,1\}$ corresponding to 
the measurement projectors $\ketbra{0}$ and $\I-\ketbra{0}$, respectively.
In this case our proxy for the full risk is
\begin{align}
    r_\text{Bernoulli}(\eps)
        &= \sum_{\data\in\{0,1\}} \Pr(d|\eps) 
            \int \Pr(\mps|d,\eps)
                (\mps-\hat{\mps}(\data,\eps))^\T \cdot Q\cdot
                (\mps-\hat{\mps}(\data,\eps))
                \dd\mps
\end{align}
noting that we only need to compute the posterior distributions 
for one outcome, since $\Pr(\data=0|\mps,\eps)+\Pr(\data=1|\mps,\eps)=1$.
Thus we have $\noutcomes=1$.
In a similar way, we could use a binomial model as a proxy for the actual
risk under the assumption 
$\argmin_\eps r(\eps)\approx \argmin_\eps r_\text{binomial}(\eps)$.

The intuition behind these Bernoulli and binomial stand-ins 
is that the most informative experiment---with
respect to parameters describing the quantum system---for
hypothetical strong measurements shouldn't be much different 
than the most informative experiment for a referenced Poisson measurement.
This reasoning is not exact, however.
This is most easily illustrated by the curves in 
\autoref{fig:effective-strong-meas}.
Here we see the mean-squared-error (of Bayes estimates) under a
binomial model and a referenced Poisson model in several regimes.
Though the MSE averaged over $p$ is roughly the same for
both models, the behavior for specific $p$ can be quite different.
We see that in relevant parameter regimes ($\alpha>1000$) that the binomial
MSE is low near the boundaries $[0,1]$ and highest in the middle,
whereas the referenced Poisson model is roughly flat across $p$.
This means that using a binomial model as a proxy would, on average, 
discourage selecting experiments whose expected value of 
$p$ is near the middle of $[0,1]$, when in fact, such experiments
might sometimes be the best ones.

Our second approach to reduce computation time of $r(e)$ is to 
approximate it through Fisher information, where the sum
over outcomes can be done analytically.
In particular, we make use of van Trees' inequality (also known
as the Bayesian Cramer-Rao bound) which gives
\begin{align}
    r(\eps) &\geq
        \frac{1}{
            \expect_{\mps\sim\Pr(\mps)} \left[
                \nabla_\mps \ell(\mps,\eps)^\T\cdot Q
                \cdot \nabla_\mps \ell(\mps,\eps)
            \right]
            +\mathcal{I}(\mps)
       =: \BCRB(\mps,\eps)
        }
\end{align}
where the left hand side is the quantity of interest, and the
right hand side is a lower bound for this quantity.
The first term of the denominator is the Fisher information
weighted by $Q$ marginalized over the current prior.
The second term of the denominator is the information of the prior, 
equal to the weighted Fisher information of the prior marginalized
over the prior, which in the
sequential Bayesian update paradigm, is the cumulative sum 
of all previous marginalized-Fisher-information-values.
Since Bayesian analysis often nearly saturates this bound,
we use the right hand side as an approximation of the left hand side.

To compute the right hand side, recall that the log-likelihood of 
our model is given by
\begin{align}
    \log\Lhood(\mps;\data,\eps)
        &= -(\beta+p(\mps,\eps)(\alpha-\beta)) 
            + \data\log(\beta+p(\mps,\eps)(\alpha-\beta))
            - \log(\data!)
\end{align}
where $p(\mps,\eps)$ is the Born rule probability of the quantum 
system, as defined in \TODO.
This produces the weighted Fisher information value $J(\mps,\eps)$,
calculated as
\begin{align}
    J(\mps,\eps) &= \expect_\data \left[
            \sum_{i,j} 
            \frac{\partial\log\Lhood}{\partial\mps_i}Q_{i,j}
            \frac{\partial\log\Lhood}{\partial\mps_j} 
        \right] \nonumber \\
        &= \sum_{i,j} Q_{i,j}
        \frac{\partial p}{\partial\mps_i}
        \frac{\partial p}{\partial\mps_j}
        \expect_\data \left[
            \frac{\partial\log\Lhood}{\partial p}
            \frac{\partial\log\Lhood}{\partial p}
        \right] \nonumber \\
        &= \sum_{i,j} Q_{i,j}
        \frac{\partial p}{\partial\mps_i}
        \frac{\partial p}{\partial\mps_j}
        \expect_\data \left[
            \left(
                \alpha-\beta-\frac{z(\alpha-\beta)}{\beta+p(\alpha-\beta)}
            \right)^2
        \right] \nonumber \\
        &= \frac{(\alpha-\beta)^2}{\beta+p(\mps,\eps)(\alpha-\beta)}
            \nabla_\mps(p(\mps,\eps))^\T\cdot Q\cdot \nabla_\mps(p(\mps,\eps))
\end{align}
where the expectation of $\data$ is just using the likelihood function
$\Pr(\data|\mps,\eps)$ and no priors.
Thus $1/\BCRB(\mps,\eps)=\expect_\mps [J(\mps,\eps)]+\mathcal{I}(\mps)$
and we have again eliminated the need to sum over distinct outcomes.
Note that $\alpha$ and $\beta$ here are components of $\mps$, so we
sum over our current knowledge of them, too.
This comes at the cost of having to compute $p(\mps,\eps)$ in addition
to its gradient, but the gradient involves no extra matrix exponentials,
and in the worst case we can use a finite different which induces 
an overhead that scales in the dimension of $\mps$.

At the end of the day, a heuristic is a heuristic, and they need to 
be tested against each other to really find the best one.

\end{document}
